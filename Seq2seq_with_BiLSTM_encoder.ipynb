{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LevTeEwpDR93"
   },
   "source": [
    "# Import libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAmAs1LxBlKV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8gUQMBxqS_Ij",
    "outputId": "217db488-c743-405c-8f6e-fbe6d2dd28a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIGTVWzXDAAy"
   },
   "source": [
    "##### INDIC_NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZzIzL5d4CC4K",
    "outputId": "5ad0bc1d-721e-47b0-8118-836d0f873b6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'indic_nlp_library'...\n",
      "remote: Enumerating objects: 1271, done.\u001b[K\n",
      "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
      "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
      "remote: Total 1271 (delta 50), reused 54 (delta 25), pack-reused 1178\u001b[K\n",
      "Receiving objects: 100% (1271/1271), 9.56 MiB | 15.39 MiB/s, done.\n",
      "Resolving deltas: 100% (654/654), done.\n",
      "Cloning into 'indic_nlp_resources'...\n",
      "remote: Enumerating objects: 133, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 133 (delta 0), reused 2 (delta 0), pack-reused 126\u001b[K\n",
      "Receiving objects: 100% (133/133), 149.77 MiB | 42.11 MiB/s, done.\n",
      "Resolving deltas: 100% (51/51), done.\n",
      "Collecting Morfessor\n",
      "  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n",
      "Installing collected packages: Morfessor\n",
      "Successfully installed Morfessor-2.0.6\n"
     ]
    }
   ],
   "source": [
    "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
    "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
    "!pip install Morfessor\n",
    "# The path to the local git repo for Indic NLP library\n",
    "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
    "# The path to the local git repo for Indic NLP Resources\n",
    "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\"\n",
    "import sys\n",
    "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
    "from indicnlp import common\n",
    "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
    "from indicnlp import loader\n",
    "loader.load()\n",
    "from indicnlp.tokenize import indic_tokenize \n",
    "from indicnlp.transliterate.unicode_transliterate import ItransTransliterator\n",
    "from indicnlp.normalize.indic_normalize import BaseNormalizer\n",
    "from indicnlp.normalize.indic_normalize import DevanagariNormalizer\n",
    "from indicnlp.morph import unsupervised_morph \n",
    "from indicnlp import common\n",
    "\n",
    "hi_analyzer=unsupervised_morph.UnsupervisedMorphAnalyzer('hi')\n",
    "from indicnlp.tokenize import indic_detokenize  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zyor8OkhDK0U"
   },
   "source": [
    "##### SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgY2gna1DHD-",
    "outputId": "62bcec25-cb2a-4b1f-9e5a-1ff61ff7e4e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.2.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
      "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZcZ3A1_LQvn",
    "outputId": "2a1ac0b8-f97f-48db-d7de-d16b24c9408e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\n",
      "\r",
      "\u001b[K     |▎                               | 10kB 26.8MB/s eta 0:00:01\r",
      "\u001b[K     |▌                               | 20kB 32.3MB/s eta 0:00:01\r",
      "\u001b[K     |▊                               | 30kB 21.4MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 40kB 24.9MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 51kB 24.0MB/s eta 0:00:01\r",
      "\u001b[K     |█▍                              | 61kB 26.6MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 71kB 20.0MB/s eta 0:00:01\r",
      "\u001b[K     |█▉                              | 81kB 20.8MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 92kB 17.5MB/s eta 0:00:01\r",
      "\u001b[K     |██▎                             | 102kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 112kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 122kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 133kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 143kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 153kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 163kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 174kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 184kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 194kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████▌                           | 204kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 215kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 225kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 235kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████▍                          | 245kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 256kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 266kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 276kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 286kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████▌                         | 296kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 307kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 317kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████▏                        | 327kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████▍                        | 337kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████▊                        | 348kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 358kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 368kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 378kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 389kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████▉                       | 399kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 409kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 419kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 430kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 440kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 450kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 460kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 471kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 481kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 491kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 501kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▎                    | 512kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▌                    | 522kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 532kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 542kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 552kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 563kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 573kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 583kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 593kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 604kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 614kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 624kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 634kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▏                 | 645kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 655kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 665kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 675kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 686kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 696kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 706kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 716kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 727kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 737kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 747kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 757kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 768kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 778kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▍              | 788kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 798kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 808kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 819kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 829kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▌             | 839kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 849kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 860kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 870kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 880kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 890kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 901kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 911kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 921kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 931kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 942kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 952kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 962kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 972kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 983kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 993kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 1.0MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 1.0MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 1.0MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 1.0MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 1.0MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▎        | 1.1MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 1.1MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 1.1MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 1.1MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 1.1MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 1.1MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▋       | 1.1MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 1.1MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 1.1MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▎      | 1.1MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 1.2MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 1.2MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 1.2MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 1.2MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 1.2MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▋     | 1.2MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 1.2MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 1.2MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 1.2MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 1.2MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 1.3MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 1.3MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 1.3MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 1.3MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 1.3MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 1.3MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 1.3MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 1.3MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 1.3MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 1.4MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 1.4MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 1.4MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 1.4MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 1.4MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 1.4MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 1.4MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 1.4MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 1.4MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 1.4MB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.5MB 18.5MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
      "Installing collected packages: nltk\n",
      "  Found existing installation: nltk 3.2.5\n",
      "    Uninstalling nltk-3.2.5:\n",
      "      Successfully uninstalled nltk-3.2.5\n",
      "Successfully installed nltk-3.6.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U nltk\n",
    "import nltk\n",
    "import sys\n",
    "nltk.download('wordnet')\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import single_meteor_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwVxK_b3kSNe"
   },
   "source": [
    "# Prepare Data for the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Bw1m62tDhbj"
   },
   "source": [
    "#### Load dataset\n",
    "Add the data from train.csv into **data** as list of lists. Each element of this list **data** is again a list of sentences, the first sentence is the *hindi sentence*,  and the second sentence is the corresponding *english sentence*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2O43uBZIw6o",
    "outputId": "6555f675-0e0d-46e9-9356-a3fdd4a4ac0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hindi', 'english']"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list()\n",
    "with open('train.csv') as csvfile:\n",
    "     spamreader = csv.reader(csvfile, delimiter=\",\")\n",
    "     for row in spamreader:\n",
    "         data.append([row[1], row[2]])\n",
    "data.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1KlUoYuDqlV"
   },
   "outputs": [],
   "source": [
    "#split into train and validation data\n",
    "train_data = data[:90000]\n",
    "validation_data = data[90000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dMZTmuv5St0O",
    "outputId": "f25c87cf-cb93-4ea0-cef7-a281b692455e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOLL7CUAR1Y8"
   },
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y28bKpn5x_l0"
   },
   "outputs": [],
   "source": [
    "en_short_forms_dict ={\"'ll\":\" will\",\n",
    "                      \"'re\":\" are\",\n",
    "                      \"i'm\":\"i am\",\n",
    "                      \"'ve\":\" have\",\n",
    "                      \"\\'ve\" :\" have\",\n",
    "                      \"\\'s\":\"'s\",\n",
    "                      \"\\'ll\":\" will\",\n",
    "                      \"\\'re\":\" are\",\n",
    "                      \"n\\'t\":\" not\" ,\n",
    "                      \" y'all\":\" you all\",\n",
    "                       \" i\\'m\":\" i am\",\n",
    "                      \"'em\":\"them\",\n",
    "                      \"can't\":\"can not\",\n",
    "                      \"won't\":\"will not\",\n",
    "                      \"cannot\":\"can not\",\n",
    "                       \"isn't\" :\"is not\",\n",
    "                       \"aren't\":\"are not\",\n",
    "                      \"wouldn't\":\"would not\",\n",
    "                      \"shouldn't\":\"should not\",\n",
    "                      \"couldn't\":\"could not\",\n",
    "                      \"wasn't\":\"was not\",\n",
    "                      \"weren't\":\"were not\",\n",
    "                      \"hasn't\":\"has not\",\n",
    "                      \"hadn't\":\"had not\",\n",
    "                      \"haven't\":\"have not\",\n",
    "                      \"'ii\":\" will\",\n",
    "                      \"fuckin'\":\"funcking\"   \n",
    "                     }\n",
    "\n",
    "hi_digits={ \"१\":\"1\",\n",
    "            \"२\":\"2\",\n",
    "            \"३\":\"3\",\n",
    "            \"४\":\"4\",\n",
    "            \"५\":\"5\",\n",
    "            \"६\":\"6\",\n",
    "            \"७\":\"7\",\n",
    "            \"८\":\"8\",\n",
    "            \"९\":\"9\",\n",
    "            \"०\":\"0\"    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7rq851C0PXV"
   },
   "outputs": [],
   "source": [
    "def check_valid_hindi_word(word):\n",
    "  alpha_num = \"abcdefghijklmnopqrstuvwxyz♪♫\"\n",
    "  for c in word:\n",
    "    if c in alpha_num or c in alpha_num.upper():\n",
    "        return False\n",
    "  return True\n",
    "\n",
    "def remove_punctuations(sentence):\n",
    "  regex = re.compile(r'[@_!♫♪#$%^&*(.,)<>?/\\|}{~:;-]')\n",
    "  sentence = regex.sub('',sentence)\n",
    "  return sentence\n",
    "\n",
    "\n",
    "def hi_tokenizer(sentence):\n",
    "  normalizer = DevanagariNormalizer(\"hi\", remove_nuktas=True)\n",
    "  sentence = normalizer.normalize(sentence)               #normalizes the sentence\n",
    "  sentence = remove_punctuations(sentence)                #removes punctuations\n",
    "  for k in hi_digits:\n",
    "      if k in sentence:\n",
    "          sentence = sentence.replace(k,hi_digits[k])\n",
    "  hi_tokens = indic_tokenize.trivial_tokenize(sentence)  #tokenization\n",
    "  for index,token in enumerate(hi_tokens):  \n",
    "    if not check_valid_hindi_word(token):   \n",
    "      hi_tokens[index] = \"<unk>\"                         #replace with <unk> if the word contains irrelevant characters\n",
    "  return hi_tokens\n",
    "\n",
    "\n",
    "def en_tokenizer(sentence):\n",
    "  sentence = remove_punctuations(sentence)                      #removes punctuations\n",
    "  sentence = sentence.lower()   \n",
    "  for key in en_short_forms_dict:\n",
    "    sentence= sentence.replace(key, en_short_forms_dict[key])   #replaces short forms with full forms\n",
    "  #li = list(token.text for token in nlp.tokenizer(sentence))    \n",
    "  li = indic_tokenize.trivial_tokenize(sentence)                #tokenization\n",
    "\n",
    "  #indic tokenizes the word \"what's\" as ['what',\"'\",'s']. So the following code changes it into ['what',\"'s\"]\n",
    "  for i in range(len(li)-1):\n",
    "     if li[i]==\"'\" and li[i+1]==\"s\":\n",
    "        li[i+1] = \"'s\"\n",
    "  while \"'\" in li:\n",
    "     li.remove(\"'\")\n",
    "     \n",
    "  return li\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mc2G4OafWeq2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V68j31Q2Wenz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ErdnFpG8TdL7"
   },
   "outputs": [],
   "source": [
    "class Lang:\n",
    "   def __init__(self):\n",
    "     self.word2index = {\"<unk>\":0 , \"<sos>\":1, \"<eos>\":2, \"<pad>\":3}\n",
    "     self.index2word = {0:\"<unk>\" , 1:\"<sos>\", 2:\"<eos>\", 3:\"<pad>\"}\n",
    "     self.vocab_size = len(self.word2index)\n",
    "     self.word_count = {\"<unk>\":1 , \"<sos>\":1, \"<eos>\":1, \"<pad>\":1}\n",
    "\n",
    "   def add_to_vocab(self,token_list):\n",
    "     for token in token_list:\n",
    "        if token not in self.word2index:        #add to vocab only if its not already present \n",
    "           ind = len(self.word2index)\n",
    "           self.word2index[token] = ind\n",
    "           self.index2word[ind] = token\n",
    "           self.vocab_size += 1\n",
    "           self.word_count[token] = 1\n",
    "        else:\n",
    "           self.word_count[token] += 1   \n",
    "   \n",
    "   def tokens2tensor(self,token_list):\n",
    "      token_indices = list()\n",
    "      \n",
    "      #for each token, append its index as per the built vocabulary. If token is not present in the vocab, append the index of <unk> \n",
    "      for token in token_list:\n",
    "          if token in self.word2index:\n",
    "              token_indices.append(self.word2index[token])\n",
    "          else:\n",
    "              token_indices.append(self.word2index[\"<unk>\"])\n",
    "\n",
    "      #convert the token_indices into tensor              \n",
    "      sentence_tensor = torch.tensor(token_indices).unsqueeze(1).to(device)\n",
    "      \n",
    "      return sentence_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GymCfvG_sNxI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unfzxs7glhau"
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(data):\n",
    "  remove_rows = list()\n",
    "  english = Lang()    #object of Lang class for english \n",
    "  hindi = Lang()      #object of Lang class for hindi\n",
    "\n",
    "  for index in range(len(data)):\n",
    "    hi_sentence = data[index][0]\n",
    "    en_sentence = data[index][1]    \n",
    "\n",
    "    #tokenize the sentences and calculate the number of tokens in each sentence\n",
    "    hi_tokens = hi_tokenizer(hi_sentence)\n",
    "    no_of_hi_tokens = len(hi_tokens)\n",
    "    en_tokens = en_tokenizer(en_sentence)\n",
    "    no_of_en_tokens = len(en_tokens)\n",
    "\n",
    "    #remove pairs if any of the sentences have with 0 length after removing punctuations or contains all <unk>\n",
    "    if (no_of_hi_tokens==0 or no_of_en_tokens==0) or hi_tokens.count(\"<unk>\")== no_of_hi_tokens :\n",
    "        remove_rows.append([hi_sentence,en_sentence])\n",
    "    else:\n",
    "        hindi.add_to_vocab(hi_tokens)     #add tokens to hindi vocab \n",
    "        english.add_to_vocab(en_tokens)   #add tokens ro english vocab\n",
    "\n",
    "  \n",
    "  print(f\"No of sentence before :: {len(data)}\")\n",
    "  for row in remove_rows:\n",
    "    data.remove(row)\n",
    "  print(f'No of sentence after :: {len(data)}')\n",
    "  return data, hindi,english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jr7KsZUQLzj0",
    "outputId": "3fa8b20c-b2c3-4841-bbbd-2f1c30967d83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of sentence before :: 90000\n",
      "No of sentence after :: 89284\n"
     ]
    }
   ],
   "source": [
    "train_data, hindi,english = data_preprocessing(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zay0UkM5THIH",
    "outputId": "8090a2f6-4908-4c21-e0e0-48c3889ee5bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi vocab size : 39023 \n",
      "English vocab size : 31159\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hindi vocab size : {hindi.vocab_size} \\nEnglish vocab size : {english.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJg_JkAAYkvc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvOhkFuoq50G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3EIXNVckyxM"
   },
   "source": [
    "#### Create Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3DnNbnNDsnC"
   },
   "outputs": [],
   "source": [
    "input_size = hindi.vocab_size\n",
    "output_size = english.vocab_size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbIbFC3KIPdU",
    "outputId": "1ba1e7a5-4880-471a-eba3-59a41e7f1c68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39023 31159\n"
     ]
    }
   ],
   "source": [
    "print(input_size,output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUPTMZrs1IhQ"
   },
   "outputs": [],
   "source": [
    "def sort_sentences(data, sort_key):\n",
    "    data_list = list()\n",
    "    for pair in data:\n",
    "        hi_sentence , en_sentence =  pair\n",
    "\n",
    "        #append <sos> as the first token and <eos> as the last token\n",
    "        hi_tokens = ['<sos>'] + hi_tokenizer(hi_sentence) + ['<eos>']\n",
    "        en_tokens = ['<sos>'] + en_tokenizer(en_sentence) + ['<eos>']\n",
    "        \n",
    "        temp_dict = {\"hi_tokens\": hi_tokens, \"en_tokens\": en_tokens, \"hi_len\":len(hi_tokens), \"en_len\":len(en_tokens)}\n",
    "        data_list.append(temp_dict)\n",
    "\n",
    "    #add extra sentences with one <unk> token just to make sure size of each batch equals batch_size.\n",
    "    #Even these sentences should include <sos> at the start and <eos> at the end \n",
    "    while len(data_list) % batch_size != 0:\n",
    "        temp_dict = {\"hi_tokens\": [\"<sos>\",\"<unk>\",\"<eos>\"], \"en_tokens\": [\"<sos>\",\"<unk>\",\"<eos>\"], \"hi_len\": 3, \"en_len\": 3}\n",
    "        data_list.insert(0,temp_dict)\n",
    "   \n",
    "    #sort according to length of hindi sentence\n",
    "    sorted_data = sorted(data_list, key= lambda x: x[\"hi_len\"])\n",
    "    return sorted_data\n",
    "\n",
    "sorted_data = sort_sentences(train_data,\"hi_len\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-RvGZvIU3wv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0liioK0HZ02K"
   },
   "outputs": [],
   "source": [
    "def create_batches(sorted_data,batch_size):     \n",
    "    batch_list = list()\n",
    "\n",
    "    #divide into batches\n",
    "    for i in range(0,len(sorted_data),batch_size):     \n",
    "        batch = sorted_data[i : i+batch_size]\n",
    "\n",
    "        #get the maximum length of sentences in the batch \n",
    "        hi_max_len = max(batch, key= lambda x: x[\"hi_len\"])[\"hi_len\"]\n",
    "        en_max_len = max(batch, key= lambda x: x[\"en_len\"])[\"en_len\"]\n",
    "        \n",
    "        temp_batch = list()\n",
    "        for item in batch:\n",
    "            hi_tokens, en_tokens, hi_len, en_len = item.values()\n",
    "            \n",
    "            #add <pad> tokens at the end of sentence (after <eos> token) to make sure each sentence in the batch has same length\n",
    "            if hi_len < hi_max_len:\n",
    "                padding = [\"<pad>\"]*(hi_max_len-len(hi_tokens))\n",
    "                item[\"hi_tokens\"] = hi_tokens + padding\n",
    "                \n",
    "            if en_len < en_max_len:\n",
    "                padding = [\"<pad>\"]*(en_max_len-len(en_tokens))\n",
    "                item[\"en_tokens\"] = en_tokens + padding\n",
    "\n",
    "            #convert token list into tensor    \n",
    "            hi_tensor = hindi.tokens2tensor(item[\"hi_tokens\"])\n",
    "            en_tensor = english.tokens2tensor(item[\"en_tokens\"])\n",
    "\n",
    "            #add the resultant tensors of both the languages into the batch\n",
    "            temp_item = [hi_tensor, en_tensor]\n",
    "            temp_batch.append(temp_item)\n",
    "        \n",
    "        #concat tensors such that the first row contains <sos> token of all sentences, 2nd row contains the 1st word of all the sentences, and so on.\n",
    "        #In other words, ith column is for ith sentence of the batch\n",
    "        batch_hi = torch.cat([x[0] for x in temp_batch], dim=1)\n",
    "        batch_eng = torch.cat([x[1] for x in temp_batch], dim=1)\n",
    "\n",
    "        #add the batch tensors into batch_list\n",
    "        batch_list.append([batch_hi, batch_eng])\n",
    "            \n",
    "    return batch_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7beauM0XlLBp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1Vr_dyHcM_m"
   },
   "outputs": [],
   "source": [
    "batch_list = create_batches(sorted_data, batch_size)\n",
    "\n",
    "# shuffle the batches\n",
    "random.shuffle(batch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLw99A_8Z0-A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tepdlefSbaKZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zc6_DjBR2-Bi"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba_SRy1wE3Zc"
   },
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3J0LzVJ3CHB"
   },
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, n_layers, dropout):\n",
    "        super(EncoderLSTM,self).__init__()\n",
    "        self.hidden_size = hidden_size          \n",
    "        self.input_size = input_size            #size of hindi vocab, i.e., unique tokens in hindi sentences \n",
    "        self.embedding_size = embedding_size    #embedding dimension \n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.bilstm = nn.LSTM(embedding_size, hidden_size,n_layers , dropout=dropout,bidirectional=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_lin = nn.Linear(hidden_size*2,hidden_size)\n",
    "        \n",
    "    def forward(self, input_sentence):\n",
    "        #input_sentence.shape = [sentence_length,batch_size]\n",
    "        embedding = self.embedding(input_sentence)   #embedding.shape = [sentence_length, batch_size, embedding_size]\n",
    "        embedding = self.dropout(embedding) \n",
    "        output, (hidden,cell)= self.bilstm(embedding)          #lstm returns 3 values: output, hidden state and cell state\n",
    "        #output.shape = [sentence_length, batch, n_directions * hidden_size]\n",
    "        #hidden.shape = [n_layers * n_directions, batch_size, hidden_size] \n",
    "\n",
    "\n",
    "        #concatenate the forward and backward hidden and cell\n",
    "        hidden=self.hidden_lin(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)).unsqueeze(0)\n",
    "        cell=self.hidden_lin(torch.cat((cell[-2,:,:], cell[-1,:,:]), dim = 1)).unsqueeze(0)\n",
    "        \n",
    "        return output,hidden,cell                        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMMZytUrFAHr"
   },
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2a6sOG2zTCn"
   },
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, n_layers,dropout):\n",
    "        super(DecoderLSTM,self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size  \n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.bilstm = nn.LSTM(embedding_size, hidden_size,n_layers , dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self, input,output, hidden,cell):\n",
    "        #input.shape = [batch_size]           \n",
    "        input = input.unsqueeze(0)                      #added 1 dimension, input.shape = [1,batch_size]  \n",
    "        embedding = self.embedding(input)               #embedding.shape = [1, batch_size , embedding_size]\n",
    "        embedding = self.dropout(embedding)     \n",
    "        output, (hidden,cell) = self.bilstm(embedding,(hidden,cell))    \n",
    "        predicted_output = self.linear(output)         \n",
    "        predicted_output = predicted_output.squeeze(0)  \n",
    "        return predicted_output,hidden,cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afvV1EVTlOHQ"
   },
   "source": [
    "#### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bID4KBeV4l2y"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,input_size,output_size,embedding_size,hidden_size,n_layers,dropout, device):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderLSTM(input_size,embedding_size,hidden_size,n_layers,dropout)#.to(device)\n",
    "        self.decoder = DecoderLSTM(output_size,embedding_size,hidden_size,output_size, n_layers,dropout)#.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self,input_sentence, target_output_sentence, teacher_forcing_ratio = 0.5):\n",
    "        input_sen_len = input_sentence.shape[0]\n",
    "        batch_size = input_sentence.shape[1]\n",
    "        output_sen_len = target_output_sentence.shape[0]\n",
    "        output_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        #tensor to store predicted words by the decoder\n",
    "        predicted_word_indexes = torch.zeros(output_sen_len, batch_size, output_vocab_size).to(self.device)\n",
    "\n",
    "        #pass the input hindi sentence into the encoder \n",
    "        encoder_output, hidden,cell = self.encoder(input_sentence)\n",
    "\n",
    "        decoder_input = target_output_sentence[0]  #first input to the decoder is always the init_token, i.e., <sos> token\n",
    "        \n",
    "        for i in range(1,output_sen_len):\n",
    "            #pass the previous word along with the hidden and cell states of encoder into the decoder\n",
    "            output,hidden,cell = self.decoder(decoder_input, encoder_output, hidden,cell)\n",
    "\n",
    "            #append the next predicted word\n",
    "            predicted_word_indexes[i] = output\n",
    "\n",
    "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "            best_word = output.argmax(1)\n",
    "            if use_teacher_forcing:\n",
    "              decoder_input = target_output_sentence[i]\n",
    "            else:\n",
    "              decoder_input = best_word\n",
    "\n",
    "        return predicted_word_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDamH_ad4C8t"
   },
   "source": [
    "# Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nrSOYMfBqQ3"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH =400\n",
    "\n",
    "#hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "hidden_size = 512\n",
    "embedding_size = 256           #same for both bilstms (encoder and decoder)\n",
    "dropout = 0.5\n",
    "n_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-vG-Aomql7Sy",
    "outputId": "beb54e8a-f2d9-49aa-db92-01d419d45bba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "#initialize the objects of Encoder, Decoder and Seq2Seq class\n",
    "model = Seq2Seq(input_size,output_size,embedding_size,hidden_size,n_layers,dropout, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "viR1g5G8GhjK",
    "outputId": "a513937b-3560-4c1b-a2a1-b6c9a899b948"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): EncoderGRU(\n",
       "    (embedding): Embedding(39023, 256)\n",
       "    (gru): LSTM(256, 512, dropout=0.5, bidirectional=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (hidden_lin): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  )\n",
       "  (decoder): DecoderGRU(\n",
       "    (embedding): Embedding(31159, 256)\n",
       "    (gru): LSTM(256, 512, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (linear): Linear(in_features=512, out_features=31159, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize the parameters\n",
    "def init_weights(model):\n",
    "    for name, parameter in model.named_parameters():\n",
    "        nn.init.uniform_(parameter.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dA6WyL76kQrD"
   },
   "outputs": [],
   "source": [
    "def train_batch_list(model, batch_list, criterion,encoder_optimizer,decoder_optimizer):\n",
    "    for batch in batch_list:\n",
    "        #get hindi and their corresponding english sentences from the batch\n",
    "        input_sentence = batch[0]\n",
    "        target_sentence = batch[1]\n",
    "      \n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()    \n",
    "\n",
    "        #pass the hindi and their corresponding english sentences into the model to get the predicted sentence     \n",
    "        predicted_sentence = model(input_sentence, target_sentence) \n",
    "\n",
    "        #adjust the shapes\n",
    "        predicted_sentence = predicted_sentence[1:].view(-1, predicted_sentence.shape[2])\n",
    "        target_sentence = target_sentence[1:].view(-1)\n",
    "\n",
    "        #calculate loss    \n",
    "        loss = criterion(predicted_sentence,target_sentence)\n",
    "        \n",
    "        #backpropagate loss\n",
    "        loss.backward()\n",
    "\n",
    "        #clip the gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        #opitimize the parameters according to the propagated loss\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgH4P1jxGwO6"
   },
   "outputs": [],
   "source": [
    "def train(model, batch_list,num_epochs=num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    #initialize the optimizer and the criterion(Loss function) to be used\n",
    "    encoder_optimizer = optim.Adam(model.encoder.parameters(),lr=learning_rate)             #using Adam optimizer for encoder \n",
    "    decoder_optimizer = optim.Adam(model.decoder.parameters(),lr=learning_rate)             #using Adam optimizer for decoder\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = english.word2index[\"<pad>\"])             #using CrossEntropyLoss function #reduction='mean'\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        loss = train_batch_list(model,batch_list,criterion,encoder_optimizer,decoder_optimizer)\n",
    "        total_loss += loss.item()\n",
    "        torch.save(model.state_dict(),\"bilstm_model.pt\" )\n",
    "        print(f'\\n\\nEpoch: {epoch+1}/{num_epochs}     Loss: {loss.item():.4f}')  \n",
    "\n",
    "    print(f\"\\n\\n Total loss ::: {total_loss/len(batch_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HpSQxuQmrIy5",
    "outputId": "011ee0dd-01db-4c0e-f1e6-acbd4ad62542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 1/20     Loss: 6.9541\n",
      "\n",
      "\n",
      "Epoch: 2/20     Loss: 6.7443\n",
      "\n",
      "\n",
      "Epoch: 3/20     Loss: 6.5040\n",
      "\n",
      "\n",
      "Epoch: 4/20     Loss: 6.4498\n",
      "\n",
      "\n",
      "Epoch: 5/20     Loss: 6.3561\n",
      "\n",
      "\n",
      "Epoch: 6/20     Loss: 6.2463\n",
      "\n",
      "\n",
      "Epoch: 7/20     Loss: 6.1775\n",
      "\n",
      "\n",
      "Epoch: 8/20     Loss: 6.0925\n",
      "\n",
      "\n",
      "Epoch: 9/20     Loss: 6.1018\n",
      "\n",
      "\n",
      "Epoch: 10/20     Loss: 6.1247\n",
      "\n",
      "\n",
      "Epoch: 11/20     Loss: 5.9588\n",
      "\n",
      "\n",
      "Epoch: 12/20     Loss: 5.8228\n",
      "\n",
      "\n",
      "Epoch: 13/20     Loss: 5.9620\n",
      "\n",
      "\n",
      "Epoch: 14/20     Loss: 5.8549\n",
      "\n",
      "\n",
      "Epoch: 15/20     Loss: 5.8397\n",
      "\n",
      "\n",
      "Epoch: 16/20     Loss: 5.8585\n",
      "\n",
      "\n",
      "Epoch: 17/20     Loss: 5.6967\n",
      "\n",
      "\n",
      "Epoch: 18/20     Loss: 6.0312\n",
      "\n",
      "\n",
      "Epoch: 19/20     Loss: 5.7251\n",
      "\n",
      "\n",
      "Epoch: 20/20     Loss: 5.6702\n",
      "\n",
      "\n",
      " Total loss ::: 0.0438\n"
     ]
    }
   ],
   "source": [
    "train(model,batch_list, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QctI3W5rF1np"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9F2Cz-k4pyw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwqawHHw4Mtx"
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "torch.save(model.state_dict(),\"bilstm_model.pt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdNoJ9AyG_Q8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7iP8kUIJoqe"
   },
   "source": [
    "# Test on Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9-9dWrO_Xtt",
    "outputId": "da403ef2-171d-4157-f1c0-fd0ac1932921"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): EncoderGRU(\n",
       "    (embedding): Embedding(39023, 256)\n",
       "    (gru): LSTM(256, 512, dropout=0.5, bidirectional=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (hidden_lin): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  )\n",
       "  (decoder): DecoderGRU(\n",
       "    (embedding): Embedding(31159, 256)\n",
       "    (gru): LSTM(256, 512, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (linear): Linear(in_features=512, out_features=31159, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 117,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the saved model\n",
    "model = Seq2Seq(input_size,output_size,embedding_size,hidden_size,n_layers,dropout, device).to(device)\n",
    "model.load_state_dict(torch.load('bilstm_model.pt',map_location=torch.device(device)))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tf1FtTrAwEtY"
   },
   "outputs": [],
   "source": [
    "validation_predictions= list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14kkDXaoodvF"
   },
   "outputs": [],
   "source": [
    "def add_punctuation_at_the_end(hi_sentence,en_sentence):\n",
    "    punc =\"\"\n",
    "    #remove extra spaces at the end\n",
    "    while en_sentence[-1] ==\" \":\n",
    "      en_sentence = en_sentence[:-1]\n",
    "      if len(en_sentence) == 0:\n",
    "        return en_sentence \n",
    "\n",
    "    if hi_sentence[-1] in \"!.?\":\n",
    "      punc = hi_sentence[-1]\n",
    "    elif hi_sentence[-1] in \"।|\":\n",
    "      punc = \".\"\n",
    "    \n",
    "    return en_sentence + punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znS5L_X1ofSd"
   },
   "outputs": [],
   "source": [
    "def translate_sentence(model, hi_sentence, hindi, english, device, max_length=400):\n",
    "    #tokenize the hindi sentence\n",
    "    hi_tokens = hi_tokenizer(hi_sentence)\n",
    "\n",
    "    #convert it into tensor\n",
    "    sentence_tensor = hindi.tokens2tensor(hi_tokens)   \n",
    "\n",
    "    with torch.no_grad():\n",
    "        #pass the source sentence into the encoder to get the hidden and cell states\n",
    "        enc_output,hidden,cell = model.encoder(sentence_tensor)\n",
    "\n",
    "    predicted_word_indices = [english.word2index[\"<sos>\"]]              #index of <sos> in english vocab\n",
    "    predicted_sentence = \"\"\n",
    "\n",
    "    #repeat until the len of predicted sentence is less than max_length or the decoder predicts <eos>\n",
    "    while len(predicted_word_indices)<max_length and predicted_word_indices[-1]!= english.word2index[\"<eos>\"]:\n",
    "        prev_word = [predicted_word_indices[-1]]             \n",
    "        prev_word = torch.tensor(prev_word).to(device)                  #convert into tensor\n",
    "        best_word = \"\"\n",
    "        with torch.no_grad():\n",
    "            '''\n",
    "             pass the last predicted word along with the hidden and cell state of the encoder\n",
    "             into the decoder to get the next predicted word\n",
    "            '''\n",
    "            output,hidden,cell = model.decoder(prev_word,enc_output, hidden,cell)\n",
    "            _ ,best_word = output.data.topk(1)                          #get the best predicted word index\n",
    "          \n",
    "        predicted_word_indices.append(best_word.item())                 #append it to the list of predicted word indices\n",
    "        predicted_sentence += english.index2word[best_word.item()]+\" \"  #append the word corresponding to the predicted index\n",
    "\n",
    "    translated_sentence = predicted_sentence.capitalize().replace(\"<sos> \",\"\").replace(\" <eos>\",\"\").replace(\" 's\",\"'s\").replace(\"[ \",\"[\").replace(\" ]\",\"]\").replace(\"( \",\"(\").replace(\" )\",\")\").replace(\" i \",\" I \")\n",
    "    return translated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SRI9wQG3cP6k"
   },
   "outputs": [],
   "source": [
    "#predict for each hindi sentence in the validation set\n",
    "for i in range(len(validation_data)):\n",
    "  #get hindi sentence\n",
    "  sentence = validation_data[i][0]\n",
    "\n",
    "  token_list = hi_tokenizer(sentence)\n",
    "  #print(token_list)\n",
    "\n",
    "  #convert it into tensor\n",
    "  sentence_tensor = hindi.tokens2tensor(sentence)\n",
    "\n",
    "  #get the translated sentence predicted by the trained model\n",
    "  predicted_sentence = translate_sentence(model,sentence,hindi,english,device,max_length=400)\n",
    "\n",
    "  #make certain replacements\n",
    "  predicted_sentence = predicted_sentence.replace(\" 's\",\"'s\").replace(\"[ \",\"[\").replace(\" ]\",\"]\").replace(\"( \",\"(\").replace(\" )\",\")\")\n",
    "  predicted_sentence = add_punctuation_at_the_end(sentence,predicted_sentence)\n",
    "\n",
    "  #append  the results\n",
    "  validation_predictions.append(predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CbTiEzNSkOV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wak0DXchDe7a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKL8BM--Jteo"
   },
   "outputs": [],
   "source": [
    "#write both predicted sentences and actual output into separate .txt files\n",
    "file = open(\"hypotheses.txt\",\"w\")\n",
    "for x in validation_predictions[:-1]:\n",
    "   file.write(x)\n",
    "   file.write(\"\\n\")\n",
    "file.write(validation_predictions[-1])\n",
    "file.close()\n",
    "\n",
    "file = open(\"references.txt\",\"w\")\n",
    "for x in validation_data[:-1]:\n",
    "   file.write(x[1])\n",
    "   file.write(\"\\n\")\n",
    "file.write(validation_data[-1][1])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pexkB9How_ts"
   },
   "source": [
    "### Run evaluation.py script to generate bleu score and meteor score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "epk9kJxvw9sQ",
    "outputId": "620837f3-75a6-4886-cd76-2211d9e2bd5c"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "\n",
    "\n",
    "file1 = open(\"hypotheses.txt\", 'r')\n",
    "references = file1.readlines()\n",
    "file2 = open(\"references.txt\", 'r')\n",
    "hypotheses = file2.readlines()\n",
    "\n",
    "# Download wordnet so that METEOR scorer works.\n",
    "\n",
    "(f'E: Number of sentences do not match. True: {len(references)} Pred: {len(hypotheses)}')\n",
    "\n",
    "if len(references) != len(hypotheses):\n",
    "    print(f'E: Number of sentences do not match. True: {len(references)} Pred: {len(hypotheses)}')\n",
    "    sys.exit()\n",
    "\n",
    "print(f'D: Number of sentences: {len(hypotheses)}')\n",
    "scores = {}\n",
    "\n",
    "# Macro-averaged BLEU-4 score.\n",
    "scores['bleu_4_macro'] = 0\n",
    "for ref, hyp in zip(references, hypotheses):\n",
    "        scores['bleu_4_macro'] += sentence_bleu(\n",
    "            [ref.split()],\n",
    "            hyp.split(),\n",
    "            smoothing_function=SmoothingFunction().method2\n",
    "        )\n",
    "scores['bleu_4_macro'] /= len(references)\n",
    "\n",
    "# BLEU-4 score.\n",
    "scores['bleu_4'] = corpus_bleu(\n",
    "        [[ref.split()] for ref in references],\n",
    "        [hyp.split() for hyp in hypotheses],\n",
    "        smoothing_function=SmoothingFunction().method2\n",
    "    )\n",
    "\n",
    "# METEOR score.\n",
    "scores['meteor'] = 0\n",
    "for ref, hyp in zip(references, hypotheses):\n",
    "        scores['meteor'] += single_meteor_score(ref, hyp)\n",
    "scores['meteor'] /= len(references)\n",
    "\n",
    "print(f'D: Scores: {scores}')\n",
    "\n",
    "# Print out scores.\n",
    "for key in scores:\n",
    "      print(f'{key}: {scores[key]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O23RXhkIDI4L"
   },
   "source": [
    "# Test on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "LwstSlILDe0v",
    "outputId": "968d196d-a305-41e9-ec7e-970a6b35040b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'hindi'"
      ]
     },
     "execution_count": 124,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_data = list()\n",
    "with open('hindistatements.csv') as csvfile:\n",
    "     spamreader = csv.reader(csvfile, delimiter=\",\")\n",
    "     for row in spamreader:\n",
    "         test_data.append(row[2])\n",
    "test_data.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sLSGU3nKL7Hq",
    "outputId": "c2485f01-8916-4ea1-cebf-f8dacde08f18"
   },
   "outputs": [],
   "source": [
    "# test_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feKABcS2dBVy"
   },
   "outputs": [],
   "source": [
    "predictions = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGjwO7GML8nT",
    "outputId": "b3f1a3f3-10c2-471f-d58d-1a8d392f829d"
   },
   "outputs": [],
   "source": [
    "for i in range(len(test_data)):\n",
    "  #get the hindi sentence\n",
    "  sentence = test_data[i]\n",
    "\n",
    "  #get the predicted translated sentence\n",
    "  predicted_sentence = translate_sentence(model,sentence,hindi,english,device,max_length=400)\n",
    "  predicted_sentence = predicted_sentence.replace(\" 's\",\"'s\").replace(\"[ \",\"[\").replace(\" ]\",\"]\").replace(\"( \",\"(\").replace(\" )\",\")\")\n",
    "  predicted_sentence = add_punctuation_at_the_end(sentence,predicted_sentence)\n",
    "  \n",
    "  #print(sentence,\"\\n\",predicted_sentence,\"\\n\\n\")\n",
    "\n",
    "  #append the results\n",
    "  predictions.append(predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6yuoVmKMTKS"
   },
   "outputs": [],
   "source": [
    "#write results into answer\n",
    "file = open(\"answer.txt\",\"w\")\n",
    "for x in predictions[:-1]:\n",
    "   file.write(x)\n",
    "   file.write(\"\\n\")\n",
    "file.write(predictions[-1])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nal4Z3B9Mqpr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxVN7_DU5D_V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1nnEWWA8SeH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UCu7Rl0DBgvu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DojduqllnEt1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdtHDYFAiHhe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgIRgEexIk9B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d42W9WUGRZzo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTKzMEkgRizW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "LevTeEwpDR93",
    "2Bw1m62tDhbj"
   ],
   "name": "Seq2seq with BiLSTM (phase4)(1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
