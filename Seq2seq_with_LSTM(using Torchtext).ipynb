{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2seq with LSTM(phase1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LevTeEwpDR93"
      },
      "source": [
        "## Import libraries and *packages*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAmAs1LxBlKV"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchtext.legacy.data import Field, BucketIterator,TabularDataset\n",
        "import random\n",
        "import re\n",
        "from torchtext import data\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIGTVWzXDAAy"
      },
      "source": [
        "##### INDIC_NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzIzL5d4CC4K",
        "outputId": "69e7c6cf-26eb-452e-fc66-43f411fd036f"
      },
      "source": [
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!pip install Morfessor\n",
        "# The path to the local git repo for Indic NLP library\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
        "# The path to the local git repo for Indic NLP Resources\n",
        "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\"\n",
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "from indicnlp import loader\n",
        "loader.load()\n",
        "from indicnlp.tokenize import indic_tokenize \n",
        "from indicnlp.transliterate.unicode_transliterate import ItransTransliterator\n",
        "from indicnlp.normalize.indic_normalize import BaseNormalizer\n",
        "from indicnlp.normalize.indic_normalize import DevanagariNormalizer\n",
        "from indicnlp.morph import unsupervised_morph \n",
        "from indicnlp import common\n",
        "\n",
        "hi_analyzer=unsupervised_morph.UnsupervisedMorphAnalyzer('hi')\n",
        "from indicnlp.tokenize import indic_detokenize  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 93, done.\u001b[K\n",
            "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 1271 (delta 50), reused 54 (delta 25), pack-reused 1178\u001b[K\n",
            "Receiving objects: 100% (1271/1271), 9.56 MiB | 15.15 MiB/s, done.\n",
            "Resolving deltas: 100% (654/654), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 133 (delta 0), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (133/133), 149.77 MiB | 41.38 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "Collecting Morfessor\n",
            "  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n",
            "Installing collected packages: Morfessor\n",
            "Successfully installed Morfessor-2.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zyor8OkhDK0U"
      },
      "source": [
        "##### SPACY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgY2gna1DHD-",
        "outputId": "eb3d5a41-b83d-410b-b50e-92084aaf8528"
      },
      "source": [
        "!python3 -m spacy download en\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZcZ3A1_LQvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76e87c27-716d-4e6f-93fa-c460c2cc82ed"
      },
      "source": [
        "!pip install -U nltk\n",
        "import nltk\n",
        "import sys\n",
        "nltk.download('wordnet')\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import single_meteor_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/1c/c0981ef85165eb739c10f2b24d7729cef066b2bc220fbd1dd0d3c67df39a/nltk-3.6.1-py3-none-any.whl (1.5MB)\n",
            "\r\u001b[K     |▎                               | 10kB 19.8MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 27.5MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 22.1MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 24.9MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 23.8MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 20.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 20.5MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81kB 21.4MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 19.5MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102kB 19.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112kB 19.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122kB 19.4MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 19.4MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143kB 19.4MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153kB 19.4MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163kB 19.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174kB 19.4MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 19.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 194kB 19.4MB/s eta 0:00:01\r\u001b[K     |████▌                           | 204kB 19.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 215kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 225kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 235kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 245kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 256kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 266kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 276kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 286kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 296kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 307kB 19.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 317kB 19.4MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 327kB 19.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 337kB 19.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 348kB 19.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 358kB 19.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 368kB 19.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 378kB 19.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 389kB 19.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 399kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 409kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 419kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 430kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 440kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 450kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 460kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 471kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 481kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 491kB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 501kB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 512kB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 522kB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 532kB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 542kB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 552kB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 563kB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 573kB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 583kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 593kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 604kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 614kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 624kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 634kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 645kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 655kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 665kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 675kB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 686kB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 696kB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 706kB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 716kB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 727kB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 737kB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 747kB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 757kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 768kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 778kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 788kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 798kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 808kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 819kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 829kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 839kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 849kB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 860kB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 870kB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 880kB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 890kB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 901kB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 911kB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 921kB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 931kB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 942kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 952kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 962kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 972kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 983kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 993kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.0MB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.0MB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.0MB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.0MB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0MB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.1MB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.1MB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1MB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.1MB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.1MB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.1MB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2MB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2MB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2MB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2MB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.2MB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.2MB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.2MB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3MB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.3MB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.3MB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.3MB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.3MB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3MB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.3MB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.3MB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.4MB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.4MB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.4MB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.4MB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4MB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.4MB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.4MB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.4MB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.4MB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 19.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lJlXhbxELtm"
      },
      "source": [
        "# Training phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwVxK_b3kSNe"
      },
      "source": [
        "## Prepare Data for the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bw1m62tDhbj"
      },
      "source": [
        "#### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1KlUoYuDqlV"
      },
      "source": [
        "data = pd.read_csv(\"train.csv\")\n",
        "data = data[[\"hindi\",\"english\"]]\n",
        "train_data = data.sample(frac=0.9)\n",
        "validation_data = data[~data.isin(train_data)].dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "iy5XG9CbIsIZ",
        "outputId": "cfd03048-d686-4cca-81c6-9c66efe1c2c3"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hindi</th>\n",
              "      <th>english</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>एल सालवाडोर मे, जिन दोनो पक्षों ने सिविल-युद्ध...</td>\n",
              "      <td>In El Salvador, both sides that withdrew from ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>मैं उनके साथ कोई लेना देना नहीं है.</td>\n",
              "      <td>I have nothing to do with them.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-हटाओ रिक.</td>\n",
              "      <td>Fuck them, Rick.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>क्योंकि यह एक खुशियों भरी फ़िल्म है.</td>\n",
              "      <td>Because it's a happy film.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The thought reaching the eyes...</td>\n",
              "      <td>The thought reaching the eyes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102317</th>\n",
              "      <td>हम यहाँ ऊपर की ओर से लड़ रहे हैं.</td>\n",
              "      <td>We're fighting uphill here.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102318</th>\n",
              "      <td>अकेले एक साल, चलो!</td>\n",
              "      <td>A year alone, come on.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102319</th>\n",
              "      <td>और जानती हैं, मेरी माँ ने हमें सिखाया...</td>\n",
              "      <td>And you know, my mother taught us ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102320</th>\n",
              "      <td>तुमनेमेरी पूरी ज़िंदगी गया .</td>\n",
              "      <td>Since I was a boy.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102321</th>\n",
              "      <td>आपको लगता है कि कुछ बड़ा पाने के लिए होगा?</td>\n",
              "      <td>Where'd you get something that big?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>102322 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    hindi                                            english\n",
              "0       एल सालवाडोर मे, जिन दोनो पक्षों ने सिविल-युद्ध...  In El Salvador, both sides that withdrew from ...\n",
              "1                     मैं उनके साथ कोई लेना देना नहीं है.                    I have nothing to do with them.\n",
              "2                                              -हटाओ रिक.                                   Fuck them, Rick.\n",
              "3                    क्योंकि यह एक खुशियों भरी फ़िल्म है.                         Because it's a happy film.\n",
              "4                        The thought reaching the eyes...                   The thought reaching the eyes...\n",
              "...                                                   ...                                                ...\n",
              "102317                  हम यहाँ ऊपर की ओर से लड़ रहे हैं.                        We're fighting uphill here.\n",
              "102318                                 अकेले एक साल, चलो!                             A year alone, come on.\n",
              "102319           और जानती हैं, मेरी माँ ने हमें सिखाया...              And you know, my mother taught us ...\n",
              "102320                       तुमनेमेरी पूरी ज़िंदगी गया .                                 Since I was a boy.\n",
              "102321         आपको लगता है कि कुछ बड़ा पाने के लिए होगा?                Where'd you get something that big?\n",
              "\n",
              "[102322 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCkcL7dVJFbT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOLL7CUAR1Y8"
      },
      "source": [
        "#### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y28bKpn5x_l0"
      },
      "source": [
        "en_short_forms_dict ={\"'ll\":\" will\",\n",
        "                      \"'re\":\" are\",\n",
        "                      \"i'm\":\"i am\",\n",
        "                      \"'ve\":\" have\",\n",
        "                      \"\\'ve\" :\" have\",\n",
        "                      \"\\'s\":\"'s\",\n",
        "                      \"\\'ll\":\" will\",\n",
        "                      \"\\'re\":\" are\",\n",
        "                      \"n\\'t\":\" not\" ,\n",
        "                      \" y'all\":\" you all\",\n",
        "                       \" i\\'m\":\" i am\",\n",
        "                      \"'em\":\"them\",\n",
        "                      \"can't\":\"can not\",\n",
        "                      \"won't\":\"will not\",\n",
        "                      \"cannot\":\"can not\",\n",
        "                       \"isn't\" :\"is not\",\n",
        "                       \"aren't\":\"are not\",\n",
        "                      \"wouldn't\":\"would not\",\n",
        "                      \"shouldn't\":\"should not\",\n",
        "                      \"couldn't\":\"could not\",\n",
        "                      \"wasn't\":\"was not\",\n",
        "                      \"weren't\":\"were not\",\n",
        "                      \"hasn't\":\"has not\",\n",
        "                      \"hadn't\":\"had not\",\n",
        "                      \"haven't\":\"have not\",\n",
        "                      \"'ii\":\" will\",\n",
        "                      \"fuckin'\":\"funcking\"   \n",
        "                     }\n",
        "\n",
        "hi_digits={ \"१\":\"1\",\n",
        "            \"२\":\"2\",\n",
        "            \"३\":\"3\",\n",
        "            \"४\":\"4\",\n",
        "            \"५\":\"5\",\n",
        "            \"६\":\"6\",\n",
        "            \"७\":\"7\",\n",
        "            \"८\":\"8\",\n",
        "            \"९\":\"9\",\n",
        "            \"०\":\"0\"    \n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7rq851C0PXV"
      },
      "source": [
        "def check_valid_hindi_word(word):\n",
        "  alpha_num = \"abcdefghijklmnopqrstuvwxyz♪♫\"\n",
        "  for c in word:\n",
        "    if c in alpha_num or c in alpha_num.upper():\n",
        "        return False\n",
        "  return True\n",
        "\n",
        "def remove_punctuations(sentence):\n",
        "  regex = re.compile(r'[@_!♫♪#$%^&*(.,)<>?/\\|}{~:-]')\n",
        "  sentence = regex.sub('',sentence)\n",
        "  return sentence\n",
        "\n",
        "\n",
        "def hi_tokenizer(sentence):\n",
        "  normalizer = DevanagariNormalizer(\"hi\", remove_nuktas=True)\n",
        "  sentence = normalizer.normalize(sentence)               #normalizes the sentence\n",
        "  sentence = remove_punctuations(sentence)                #removes punctuations\n",
        "  for k in hi_digits:\n",
        "      if k in sentence:\n",
        "          sentence = sentence.replace(k,hi_digits[k])\n",
        "  hi_tokens = indic_tokenize.trivial_tokenize(sentence)  #tokenization\n",
        "  for index,token in enumerate(hi_tokens):  \n",
        "    if not check_valid_hindi_word(token):   \n",
        "      hi_tokens[index] = \"<unk>\"                         #replace with <unk> if the word contains irrelevant characters\n",
        "  return hi_tokens\n",
        "\n",
        "def en_tokenizer(sentence):\n",
        "  sentence = remove_punctuations(sentence)                       #removes punctuations\n",
        "  sentence = sentence.lower()   \n",
        "  for key in en_short_forms_dict:\n",
        "    sentence= sentence.replace(key, en_short_forms_dict[key])   #replaces short forms with full forms\n",
        "  li = list(token.text for token in nlp.tokenizer(sentence))    #tokenization\n",
        "  return li\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GymCfvG_sNxI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unfzxs7glhau"
      },
      "source": [
        "def data_preprocessing(data):\n",
        "  remove_rows = list()\n",
        "  for index in range(len(data)):\n",
        "    hi_sentence = data.iloc[index][\"hindi\"]\n",
        "    en_sentence = data.iloc[index][\"english\"]\n",
        "    \n",
        "    hi_tokens = hi_tokenizer(hi_sentence)\n",
        "    no_of_hi_tokens = len(hi_tokens)\n",
        "    en_tokens = en_tokenizer(en_sentence)\n",
        "    no_of_en_tokens = len(en_tokens)\n",
        "\n",
        "    remove =False\n",
        "    #remove pairs if any of the sentences have with 0 length after removing punctuations\n",
        "    if (no_of_hi_tokens==0 or no_of_en_tokens==0) :\n",
        "       remove=True\n",
        "\n",
        "    remove_rows.append(remove)  \n",
        "  \n",
        "  data[\"remove_row\"]=remove_rows\n",
        "  data.drop(data[data[\"remove_row\"]==True].index, axis=0, inplace=True)\n",
        "  data.drop(columns=\"remove_row\",inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr7KsZUQLzj0"
      },
      "source": [
        "data_preprocessing(train_data)\n",
        "train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44ckhKV4qxlX"
      },
      "source": [
        "#### Load train and validation data into csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swMH6uGPo08G"
      },
      "source": [
        "validation_data.to_csv(\"validation.csv\", index=None)\n",
        "train_data.to_csv(\"preprocessed_train.csv\",index=None)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3EIXNVckyxM"
      },
      "source": [
        "#### Create Field and build vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyBHNLkvDstu"
      },
      "source": [
        "hi_field = Field(tokenize = hi_tokenizer, init_token = \"<sos>\", eos_token = \"<eos>\",unk_token=\"<unk>\", pad_token=\"<pad>\",lower = True)\n",
        "en_field = Field(tokenize = en_tokenizer,  init_token = \"<sos>\", eos_token = \"<eos>\",unk_token=\"<unk>\", pad_token=\"<pad>\", lower = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOlf_KzRDsrt"
      },
      "source": [
        "#load preprocessed training data\n",
        "train_dataset =  TabularDataset(path=\"preprocessed_train.csv\",format=\"csv\", fields=[(\"hi\",hi_field),(\"en\",en_field)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DgjrYLLDspX"
      },
      "source": [
        "hi_field.build_vocab(train_dataset)   #create hindi vocab containing unique hindi tokens from hindi sentences in the training data\n",
        "en_field.build_vocab(train_dataset)   #create english vocab containing unique english tokens from english sentences in the training data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3DnNbnNDsnC"
      },
      "source": [
        "input_size = len(hi_field.vocab)\n",
        "output_size = len(en_field.vocab)\n",
        "batch_size = 32  \n",
        "train_iter = BucketIterator(dataset = train_dataset , batch_size=batch_size,sort_within_batch=True, device=device, sort_key= lambda x:len(x.hi))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbIbFC3KIPdU"
      },
      "source": [
        "print(input_size,output_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc6_DjBR2-Bi"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba_SRy1wE3Zc"
      },
      "source": [
        "#### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3J0LzVJ3CHB"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.hidden_size = hidden_size          \n",
        "        self.input_size = input_size            #size of hindi vocab, i.e., unique tokens in hindi sentences \n",
        "        self.embedding_size = embedding_size    #embedding dimension \n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size,num_layers , dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input_sentence):\n",
        "        embedding = self.embedding(input_sentence)\n",
        "        embedding = self.dropout(embedding)  \n",
        "        output,(hidden,cell)= self.lstm(embedding)  #lstm returns 3 values: output, hidden state and the cell state\n",
        "        return (hidden,cell) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMMZytUrFAHr"
      },
      "source": [
        "#### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVxMC_fU4BaB"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers,dropout):\n",
        "        super(Decoder,self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = input_size\n",
        "        self.embedding_size = embedding_size  \n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size,num_layers , dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(hidden_size,output_size)\n",
        "        \n",
        "    def forward(self, input, hidden, cell):           \n",
        "        input = input.unsqueeze(0)   #added 1 dimension ,since shape(input)= (batch_size) but we want (1,batch_size)  \n",
        "        embedding = self.embedding(input)\n",
        "        embedding = self.dropout(embedding)   \n",
        "        output,(hidden,cell) = self.lstm(embedding,(hidden,cell)) \n",
        "        predicted_output = self.linear(output)          #shape(predicted_output) = (1,batch_size,len(en_field.vocab))\n",
        "        predicted_output = predicted_output.squeeze(0)  #remove 1 dimension such that shape(predicted_output) = (batch_size,len(en_field.vocab))\n",
        "        return predicted_output,(hidden,cell)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afvV1EVTlOHQ"
      },
      "source": [
        "#### Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bID4KBeV4l2y"
      },
      "source": [
        "#class for the seq2seq model\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self,input_sentence, target_output_sentence, teacher_forcing_ratio = 0.5):\n",
        "        batch_size = input_sentence.shape[1]\n",
        "        input_sen_len = input_sentence.shape[0]\n",
        "        output_sen_len = target_output_sentence.shape[0]\n",
        "        output_vocab_size = self.decoder.output_size\n",
        "        \n",
        "        #tensor to store predicted words by the decoder\n",
        "        predicted_word_indexes = torch.zeros(output_sen_len, batch_size, output_vocab_size).to(self.device)\n",
        "\n",
        "        #pass the input hindi sentence into the encoder \n",
        "        hidden,cell = self.encoder(input_sentence)\n",
        "                \n",
        "        decoder_input = target_output_sentence[0]  #first input to the decoder is always the init_token, i.e., <sos> token\n",
        "        \n",
        "        for i in range(1,output_sen_len):\n",
        "            #pass the previous word along with the hidden and cell states of encoder into the decoder\n",
        "            output,(hidden,cell) = self.decoder(decoder_input, hidden, cell)\n",
        "\n",
        "            #append the next predicted word\n",
        "            predicted_word_indexes[i] = output\n",
        "\n",
        "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "            best_word = output.argmax(1)\n",
        "            if use_teacher_forcing:\n",
        "              decoder_input = target_output_sentence[i]\n",
        "            else:\n",
        "              decoder_input = best_word\n",
        "\n",
        "        return predicted_word_indexes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDamH_ad4C8t"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nrSOYMfBqQ3"
      },
      "source": [
        "MAX_LENGTH =400\n",
        "\n",
        "#hyperparameters\n",
        "num_epochs = 20\n",
        "learning_rate = 0.001\n",
        "hidden_size = 512\n",
        "embedding_size = 256           #same for both lstms (encoder and decoder)\n",
        "dropout = 0.5\n",
        "num_layers = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG9zkMm7nVHr"
      },
      "source": [
        "def hi_sentence2tensor(sentence):\n",
        "    hi_sentence_tokens =  hi_tokenizer(sentence)         #tokenization\n",
        "    hi_token_index = list()\n",
        "    for token in hi_sentence_tokens:\n",
        "       hi_token_index.append(hi_field.vocab.stoi[token])  #represent tokens from their respective indices as per vocab created for hindi\n",
        "    \n",
        "    hi_token_index.insert(0, hi_field.vocab.stoi[\"<sos>\"])     #insert token index of <sos> at start\n",
        "    hi_token_index.append(hi_field.vocab.stoi[\"<eos>\"])        #append token index of <eos> at the end\n",
        "\n",
        "    #create tensor\n",
        "    hi_sentence_tensor = torch.tensor(hi_token_index).unsqueeze(1).to(device)\n",
        "\n",
        "    return hi_sentence_tensor "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8bsJpxmmzSP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vG-Aomql7Sy"
      },
      "source": [
        "#initialize the objects of Encoder, Decoder and Seq2Seq class\n",
        "encoder = Encoder(input_size,embedding_size,hidden_size,num_layers,dropout).to(device)\n",
        "decoder = Decoder(output_size,embedding_size,hidden_size,output_size, num_layers,dropout).to(device)\n",
        "model = Seq2Seq(encoder,decoder,device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viR1g5G8GhjK"
      },
      "source": [
        "#initialize the parameters\n",
        "def init_weights(model):\n",
        "    for name, parameter in model.named_parameters():\n",
        "        nn.init.uniform_(parameter.data, -0.08, 0.08)\n",
        "            \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZklXX1v8GyCY"
      },
      "source": [
        "def translate_sentence(model, sentence_tensor, hi_field, en_field, device, max_length=400):     \n",
        "    with torch.no_grad():\n",
        "        #pass the source sentence into the encoder to get the hidden and cell states\n",
        "        hidden,cell = model.encoder(sentence_tensor)\n",
        "\n",
        "    predicted_word_indices = [en_field.vocab.stoi[en_field.init_token]]  #index of <sos> in english vocab\n",
        "    predicted_sentence = \"\"\n",
        "    while len(predicted_word_indices)<max_length and predicted_word_indices[-1]!= en_field.vocab.stoi[en_field.eos_token]:\n",
        "        prev_word = [predicted_word_indices[-1]]             \n",
        "        prev_word = torch.tensor(prev_word).to(device)  #convert into tensor\n",
        "        best_word = \"\"\n",
        "        with torch.no_grad():\n",
        "            '''\n",
        "             pass the last predicted word along with the hidden and cell state of the encoder\n",
        "             into the decoder to get the next predicted word\n",
        "            '''\n",
        "            output,(hidden, cell) = model.decoder(prev_word, hidden, cell)\n",
        "            _ ,best_word = output.data.topk(1)                    #get the best predicted word index\n",
        "        predicted_word_indices.append(best_word.item())           #append it to the list of predicted word indices\n",
        "        predicted_sentence += en_field.vocab.itos[best_word]+\" \"  #append the word corresponding to the predicted index\n",
        "\n",
        "    translated_sentence = predicted_sentence.replace(\"<sos> \",\"\").replace(\"<eos>\",\"\").replace(\" 's\",\"'s\").replace(\"[ \",\"[\").replace(\" ]\",\"]\").replace(\"( \",\"(\").replace(\" )\",\")\")\n",
        "    return translated_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA6WyL76kQrD"
      },
      "source": [
        "def train_iterator(model, iterator, criterion,encoder_optimizer,decoder_optimizer):\n",
        "    for batch in iterator:\n",
        "        #get hindi and their corresponding english sentences from the batch\n",
        "        input_sentence = batch.hi\n",
        "        target_sentence = batch.en\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()    \n",
        "\n",
        "        #pass the hindi and their corresponding english sentences into the model to get the predicted sentence     \n",
        "        predicted_sentence = model(input_sentence, target_sentence) \n",
        "\n",
        "        #adjust the shapes\n",
        "        predicted_sentence = predicted_sentence[1:].view(-1, predicted_sentence.shape[2])\n",
        "        target_sentence = target_sentence[1:].view(-1)\n",
        "\n",
        "        #calculate loss    \n",
        "        loss = criterion(predicted_sentence,target_sentence)\n",
        "        \n",
        "        #backpropagate loss\n",
        "        loss.backward()\n",
        "\n",
        "        #clip the gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "        #opitimize the parameters according to the propagated loss\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "    return loss "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI-vvdVSLFD-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgH4P1jxGwO6"
      },
      "source": [
        "def train(model, iterator,num_epochs=num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    #initialize the optimizer and the criterion(Loss function) to be used\n",
        "    encoder_optimizer = optim.RMSprop(model.encoder.parameters(),lr=learning_rate)          #using RMSProp optimizer for encoder \n",
        "    decoder_optimizer = optim.Adam(model.decoder.parameters(),lr=learning_rate)             #using Adam optimizer for decoder\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index = en_field.vocab.stoi[en_field.pad_token]) #using CrossEntropyLoss function\n",
        "    \n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        model.train()\n",
        "\n",
        "        loss = train_iterator(model,iterator,criterion,encoder_optimizer,decoder_optimizer)\n",
        "        total_loss += loss.item()\n",
        "        torch.save(model.state_dict(),\"lstm_model.pt\" )\n",
        "        print(f'\\n\\nEpoch: {epoch+1}/{num_epochs}     Loss: {loss.item():.4f}')  \n",
        "\n",
        "    print(f\"\\n\\n Total loss ::: {total_loss/len(train_iter):.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpSQxuQmrIy5"
      },
      "source": [
        "train(model, train_iter, num_epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwqawHHw4Mtx"
      },
      "source": [
        "torch.save(model.state_dict(),\"lstm_model.pt\" )\n",
        "torch.save(encoder.state_dict(),\"encoder.pt\" )\n",
        "torch.save(decoder.state_dict(),\"decoder.pt\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdNoJ9AyG_Q8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7iP8kUIJoqe"
      },
      "source": [
        "# Test on Validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPWaUvRN3Y-o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObLnm-cjcWz3"
      },
      "source": [
        "validation_data = pd.read_csv(\"validation.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9-9dWrO_Xtt"
      },
      "source": [
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "model.load_state_dict(torch.load('lstm_model.pt'))\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf1FtTrAwEtY"
      },
      "source": [
        "validation_predictions = pd.DataFrame(columns=[\"hindi\",\"english\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRI9wQG3cP6k"
      },
      "source": [
        "#predict for each hindi sentence in the validation set\n",
        "for i in range(len(validation_data)):\n",
        "  #get hindi sentence\n",
        "  sentence = validation_data.iloc[i][\"hindi\"]\n",
        "\n",
        "  #tokenize and convert it into tensor\n",
        "  sentence_tensor = hi_sentence2tensor(sentence)\n",
        "\n",
        "  #get the translated sentence predicted by the trained model\n",
        "  predicted_sentence = translate_sentence(model,sentence_tensor,hi_field,en_field,device,max_length=400)\n",
        "\n",
        "  #make certain replacements\n",
        "  predicted_sentence = predicted_sentence.replace(\" 's\",\"'s\").replace(\"[ \",\"[\").replace(\" ]\",\"]\").replace(\"( \",\"(\").replace(\" )\",\")\")\n",
        "  \n",
        "  #append  the results\n",
        "  validation_predictions =validation_predictions.append({\"hindi\":sentence,\"english\":predicted_sentence},ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKL8BM--Jteo"
      },
      "source": [
        "#write both predicted sentences and actual output into separate .txt files\n",
        "file = open(\"hypotheses.txt\",\"w\")\n",
        "for x in validation_predictions.iloc[:-1][\"english\"]:\n",
        "   file.write(x)\n",
        "   file.write(\"\\n\")\n",
        "file.write(validation_predictions.iloc[-1][\"english\"])\n",
        "file.close()\n",
        "\n",
        "file = open(\"references.txt\",\"w\")\n",
        "for x in validation_data.iloc[:-1][\"english\"]:\n",
        "   file.write(x)\n",
        "   file.write(\"\\n\")\n",
        "file.write(validation_data.iloc[-1][\"english\"])\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pexkB9How_ts"
      },
      "source": [
        "### Run evaluation.py script to generate bleu score and meteor score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epk9kJxvw9sQ"
      },
      "source": [
        "import nltk\n",
        "import sys\n",
        "nltk.download('wordnet')\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "\n",
        "file1 = open(\"hypotheses.txt\", 'r')\n",
        "references = file1.readlines()\n",
        "file2 = open(\"references.txt\", 'r')\n",
        "hypotheses = file2.readlines()\n",
        "\n",
        "total_num = len(references)\n",
        "total_bleu_scores = 0\n",
        "total_meteor_scores = 0\n",
        "for i in range(total_num):\n",
        "  total_bleu_scores+=sentence_bleu([references[i].split(\" \")], hypotheses[i].split(\" \"))\n",
        "  total_meteor_scores+=single_meteor_score(references[i], hypotheses[i])\n",
        "\n",
        "bleu_result = total_bleu_scores/total_num\n",
        "meteor_result = total_meteor_scores/total_num\n",
        "\n",
        "print(\"bleu score: \",bleu_result)\n",
        "print(\"meteor score: \",meteor_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O23RXhkIDI4L"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwstSlILDe0v"
      },
      "source": [
        "test_data = pd.read_csv(\"hindistatements.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLSGU3nKL7Hq"
      },
      "source": [
        "test_data = test_data[\"hindi\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feKABcS2dBVy"
      },
      "source": [
        "predictions = pd.DataFrame(columns=[\"hindi\",\"english\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGjwO7GML8nT"
      },
      "source": [
        "for i in range(len(test_data)):\n",
        "  #get the hindi sentence\n",
        "  sentence = test_data.iloc[i]\n",
        "\n",
        "  #tokenize the sentence and convert it into tensor\n",
        "  sentence_tensor = hi_sentence2tensor(sentence)\n",
        "\n",
        "  #get the predicted translated sentence\n",
        "  predicted_sentence = translate_sentence(model,sentence_tensor,hi_field,en_field,device,max_length=400)\n",
        "  predicted_sentence = predicted_sentence.replace(\" 's\",\"'s\").replace(\"[ \",\"[\").replace(\" ]\",\"]\").replace(\"( \",\"(\").replace(\" )\",\")\")\n",
        "  print(sentence,\"\\n\",predicted_sentence,\"\\n\\n\")\n",
        "\n",
        "  #append the results\n",
        "  predictions =predictions.append({\"hindi\":sentence,\"english\":predicted_sentence},ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6yuoVmKMTKS"
      },
      "source": [
        "#write results into answer\n",
        "file = open(\"answer.txt\",\"w\")\n",
        "for x in predictions.iloc[:4999][\"english\"]:\n",
        "   file.write(x)\n",
        "   file.write(\"\\n\")\n",
        "file.write(predictions.iloc[4999][\"english\"])\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nal4Z3B9Mqpr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}