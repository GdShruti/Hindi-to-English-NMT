{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LevTeEwpDR93"
   },
   "source": [
    "# Import libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAmAs1LxBlKV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rl_PhqKKsFJJ",
    "outputId": "e666e876-6085-436e-f2b9-2cc31d83ff22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIGTVWzXDAAy"
   },
   "source": [
    "##### INDIC_NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZzIzL5d4CC4K",
    "outputId": "fd958a21-1b36-4139-f139-96beea33da73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'indic_nlp_library'...\n",
      "remote: Enumerating objects: 1271, done.\u001b[K\n",
      "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
      "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
      "remote: Total 1271 (delta 50), reused 54 (delta 25), pack-reused 1178\u001b[K\n",
      "Receiving objects: 100% (1271/1271), 9.56 MiB | 14.61 MiB/s, done.\n",
      "Resolving deltas: 100% (654/654), done.\n",
      "Cloning into 'indic_nlp_resources'...\n",
      "remote: Enumerating objects: 133, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 133 (delta 0), reused 2 (delta 0), pack-reused 126\u001b[K\n",
      "Receiving objects: 100% (133/133), 149.77 MiB | 31.21 MiB/s, done.\n",
      "Resolving deltas: 100% (51/51), done.\n",
      "Collecting Morfessor\n",
      "  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n",
      "Installing collected packages: Morfessor\n",
      "Successfully installed Morfessor-2.0.6\n"
     ]
    }
   ],
   "source": [
    "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
    "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
    "!pip install Morfessor\n",
    "# The path to the local git repo for Indic NLP library\n",
    "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
    "# The path to the local git repo for Indic NLP Resources\n",
    "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\"\n",
    "import sys\n",
    "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
    "from indicnlp import common\n",
    "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
    "from indicnlp import loader\n",
    "loader.load()\n",
    "from indicnlp.tokenize import indic_tokenize \n",
    "from indicnlp.transliterate.unicode_transliterate import ItransTransliterator\n",
    "from indicnlp.normalize.indic_normalize import BaseNormalizer\n",
    "from indicnlp.normalize.indic_normalize import DevanagariNormalizer\n",
    "from indicnlp.morph import unsupervised_morph \n",
    "from indicnlp import common\n",
    "\n",
    "hi_analyzer=unsupervised_morph.UnsupervisedMorphAnalyzer('hi')\n",
    "from indicnlp.tokenize import indic_detokenize  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zyor8OkhDK0U"
   },
   "source": [
    "##### SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgY2gna1DHD-",
    "outputId": "36e70559-d16f-49f1-e31d-4d35f9293e76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (56.0.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
      "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZcZ3A1_LQvn",
    "outputId": "2603261e-cbb5-4516-febc-d975a43361f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 7.7MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
      "Installing collected packages: nltk\n",
      "  Found existing installation: nltk 3.2.5\n",
      "    Uninstalling nltk-3.2.5:\n",
      "      Successfully uninstalled nltk-3.2.5\n",
      "Successfully installed nltk-3.6.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U nltk\n",
    "import nltk\n",
    "import sys\n",
    "nltk.download('wordnet')\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import single_meteor_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwVxK_b3kSNe"
   },
   "source": [
    "# Prepare Data for the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Bw1m62tDhbj"
   },
   "source": [
    "### Load dataset\n",
    "Add the data from train.csv into **data** as list of lists. Each element of this list **data** is again a list of sentences, the first sentence is the *hindi sentence*,  and the second sentence is the corresponding *english sentence*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2O43uBZIw6o",
    "outputId": "9a8e07b0-f058-4ff8-ff8d-d7ee35389197"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hindi', 'english']"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list()\n",
    "with open('train.csv') as csvfile:\n",
    "     spamreader = csv.reader(csvfile, delimiter=\",\")\n",
    "     for row in spamreader:\n",
    "         data.append([row[1], row[2]])\n",
    "data.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1KlUoYuDqlV"
   },
   "outputs": [],
   "source": [
    "#split into train and validation data\n",
    "train_data = data[:-1]               #The model has been trained on the entire dataset for the test phase\n",
    "validation_data = data[90000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOLL7CUAR1Y8"
   },
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y28bKpn5x_l0"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The sentences in target language (English) contain many contracted verb forms.\n",
    "So we can replace them with their respective full forms. This will some how reduce the vocabulary size.\n",
    "For example 'will', 'she'll' and 'she' would be three different words if we use them as they are,\n",
    "but on replacing the short forms, we will have only two words 'she' and 'will'.\n",
    "Also we can replace the Devanagiri numerals by their respective Western Arabic numeral form.\n",
    "\"\"\"\n",
    "en_short_forms_dict ={\"'ll\":\" will\",\n",
    "                      \"'re\":\" are\",\n",
    "                      \"i'm\":\"i am\",\n",
    "                      \"'ve\":\" have\",\n",
    "                      \"\\'ve\" :\" have\",\n",
    "                      \"\\'s\":\"'s\",\n",
    "                      \"\\'ll\":\" will\",\n",
    "                      \"\\'re\":\" are\",\n",
    "                      \"n\\'t\":\"n't\" ,\n",
    "                      \" y'all\":\" you all\",\n",
    "                       \" i\\'m\":\" i am\",\n",
    "                      \"'em\":\"them\",\n",
    "                      \"can't\":\"can not\",\n",
    "                      \"won't\":\"will not\",\n",
    "                      \"cannot\":\"can not\",\n",
    "                       \"isn't\" :\"is not\",\n",
    "                       \"aren't\":\"are not\",\n",
    "                      \"wouldn't\":\"would not\",\n",
    "                      \"shouldn't\":\"should not\",\n",
    "                      \"couldn't\":\"could not\",\n",
    "                      \"wasn't\":\"was not\",\n",
    "                      \"weren't\":\"were not\",\n",
    "                      \"hasn't\":\"has not\",\n",
    "                      \"hadn't\":\"had not\",\n",
    "                      \"haven't\":\"have not\",\n",
    "                      \"'ii\":\" will\",\n",
    "                      \"fuckin'\":\"funcking\"   \n",
    "                     }\n",
    "\n",
    "hi_digits={ \"१\":\"1\",\n",
    "            \"२\":\"2\",\n",
    "            \"३\":\"3\",\n",
    "            \"४\":\"4\",\n",
    "            \"५\":\"5\",\n",
    "            \"६\":\"6\",\n",
    "            \"७\":\"7\",\n",
    "            \"८\":\"8\",\n",
    "            \"९\":\"9\",\n",
    "            \"०\":\"0\"    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7rq851C0PXV"
   },
   "outputs": [],
   "source": [
    "#the function checks if the hindi word contains any english or special charaters\n",
    "def check_valid_hindi_word(word):\n",
    "  alpha_num = \"abcdefghijklmnopqrstuvwxyz♪♫\"\n",
    "  for c in word:\n",
    "    if c in alpha_num or c in alpha_num.upper():\n",
    "        return False\n",
    "  return True\n",
    "\n",
    "\n",
    "#the function is used for removing certain punctuations\n",
    "def remove_punctuations(sentence):\n",
    "  regex = re.compile(r'[@_!♫♪#$%^&*(.,)<>?/\\|}{~:;-]')\n",
    "  sentence = regex.sub('',sentence)\n",
    "  return sentence\n",
    "\n",
    "\n",
    "#the function is used for tokenizing the hindi sentences\n",
    "def hi_tokenizer(sentence):\n",
    "  normalizer = DevanagariNormalizer(\"hi\", remove_nuktas=True)\n",
    "  sentence = normalizer.normalize(sentence)               #normalizes the sentence\n",
    "  sentence = remove_punctuations(sentence)                #removes punctuations\n",
    "  for k in hi_digits:\n",
    "      if k in sentence:\n",
    "          sentence = sentence.replace(k,hi_digits[k])     #replaces the Devanagiri digits with its Western Arabic form\n",
    "  hi_tokens = indic_tokenize.trivial_tokenize(sentence)   #tokenization\n",
    "  for index,token in enumerate(hi_tokens):  \n",
    "    if not check_valid_hindi_word(token):   \n",
    "      hi_tokens[index] = \"<unk>\"                          #replace with <unk> if the word contains irrelevant characters\n",
    "  return hi_tokens\n",
    "\n",
    "\n",
    "#the function is used for tokenizing the english sentences\n",
    "def en_tokenizer(sentence):\n",
    "  sentence = remove_punctuations(sentence)                      #removes punctuations\n",
    "  sentence = sentence.lower()   \n",
    "  for key in en_short_forms_dict:\n",
    "    sentence= sentence.replace(key, en_short_forms_dict[key])   #replaces short forms with their full forms\n",
    "    \n",
    "  li = indic_tokenize.trivial_tokenize(sentence)                #tokenization\n",
    "\n",
    "  #indic tokenizes the word \"what's\" as ['what',\"'\",'s']. So the following code changes it into ['what',\"'s\"]\n",
    "  for i in range(len(li)-1):\n",
    "     if li[i]==\"'\" and li[i+1]==\"s\":\n",
    "        li[i+1] = \"'s\"\n",
    "  while \"'\" in li:\n",
    "     li.remove(\"'\")\n",
    "     \n",
    "  return li\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ErdnFpG8TdL7"
   },
   "outputs": [],
   "source": [
    "class Lang:\n",
    "   def __init__(self):\n",
    "     self.word2index = {\"<unk>\":0 , \"<sos>\":1, \"<eos>\":2, \"<pad>\":3}\n",
    "     self.index2word = {0:\"<unk>\" , 1:\"<sos>\", 2:\"<eos>\", 3:\"<pad>\"}\n",
    "     self.vocab_size = len(self.word2index)\n",
    "     self.word_count = {\"<unk>\":1 , \"<sos>\":1, \"<eos>\":1, \"<pad>\":1}\n",
    "\n",
    "   def add_to_vocab(self,token_list):\n",
    "     for token in token_list:\n",
    "        if token not in self.word2index:        #add to vocab only if its not already present \n",
    "           ind = len(self.word2index)\n",
    "           self.word2index[token] = ind\n",
    "           self.index2word[ind] = token          \n",
    "           self.vocab_size += 1                 #increment the vocab_size\n",
    "           self.word_count[token] = 1\n",
    "        else:\n",
    "           self.word_count[token] += 1   \n",
    "   \n",
    "\n",
    "   def tokens2tensor(self,token_list):\n",
    "      token_indices = list()\n",
    "      #for each token, append its index as per the built vocabulary.\n",
    "      #If token is not present in the vocab, append the index of <unk> \n",
    "      for token in token_list:\n",
    "          if token in self.word2index:\n",
    "              token_indices.append(self.word2index[token])\n",
    "          else:\n",
    "              token_indices.append(self.word2index[\"<unk>\"])\n",
    "\n",
    "      #convert the token_indices into tensor              \n",
    "      sentence_tensor = torch.tensor(token_indices).unsqueeze(1).to(device)\n",
    "      \n",
    "      return sentence_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unfzxs7glhau"
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(data):\n",
    "  remove_rows = list()\n",
    "  english = Lang()    #object of Lang class for english \n",
    "  hindi = Lang()      #object of Lang class for hindi\n",
    "\n",
    "  for index in range(len(data)):\n",
    "    hi_sentence = data[index][0]\n",
    "    en_sentence = data[index][1]    \n",
    "\n",
    "    #tokenize the sentences and calculate the number of tokens in each sentence\n",
    "    hi_tokens = hi_tokenizer(hi_sentence)\n",
    "    no_of_hi_tokens = len(hi_tokens)\n",
    "    en_tokens = en_tokenizer(en_sentence)\n",
    "    no_of_en_tokens = len(en_tokens)\n",
    "\n",
    "    #remove pairs if any of the sentences have with 0 length after removing punctuations or contains all <unk>\n",
    "    if (no_of_hi_tokens==0 or no_of_en_tokens==0) or hi_tokens.count(\"<unk>\")== no_of_hi_tokens :\n",
    "        remove_rows.append([hi_sentence,en_sentence])\n",
    "    else:\n",
    "        hindi.add_to_vocab(hi_tokens)     #add tokens to hindi vocab \n",
    "        english.add_to_vocab(en_tokens)   #add tokens ro english vocab\n",
    "\n",
    "  \n",
    "  print(f\"No of sentence before :: {len(data)}\")\n",
    "  for row in remove_rows:\n",
    "    data.remove(row)\n",
    "  print(f'No of sentence after :: {len(data)}')\n",
    "  return data, hindi,english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jr7KsZUQLzj0",
    "outputId": "a99640e2-b3f6-4108-d256-45c46eef9de1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of sentence before :: 102321\n",
      "No of sentence after :: 101493\n"
     ]
    }
   ],
   "source": [
    "train_data, hindi,english = data_preprocessing(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zay0UkM5THIH",
    "outputId": "f4948cb4-186c-4cbc-b52c-bc3e47387c7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi vocab size : 41722 \n",
      "English vocab size : 32954\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hindi vocab size : {hindi.vocab_size} \\nEnglish vocab size : {english.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJg_JkAAYkvc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvOhkFuoq50G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3EIXNVckyxM"
   },
   "source": [
    "### Create Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nb4N06rbgjq8"
   },
   "source": [
    "Each sentence of train_data is first tokenized according to the defined tokenization functions for each of the languages to get a list of tokens. The *init_token* token is then appended at the start and *eos_token*  token is appended at the end of the token lists. A dictionary *temp_dict* stores these token lists for each pair along with the number of tokens. This dictionary is then sorted according to the number of tokens in *hindi* sentences.  The sorted data is then used for creating batches. The advantage of sorting the data is that minimum padding would be required while creating the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3DnNbnNDsnC"
   },
   "outputs": [],
   "source": [
    "input_size = hindi.vocab_size\n",
    "output_size = english.vocab_size\n",
    "batch_size = 32  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbIbFC3KIPdU",
    "outputId": "7f71de16-5b3b-4e43-f43b-445ca97b9bf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41722 32954\n"
     ]
    }
   ],
   "source": [
    "print(input_size,output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUPTMZrs1IhQ"
   },
   "outputs": [],
   "source": [
    "def sort_sentences(data, sort_key):\n",
    "    data_list = list()\n",
    "    for pair in data:\n",
    "        hi_sentence , en_sentence =  pair\n",
    "\n",
    "        #append <sos> as the first token and <eos> as the last token\n",
    "        hi_tokens = ['<sos>'] + hi_tokenizer(hi_sentence) + ['<eos>']\n",
    "        en_tokens = ['<sos>'] + en_tokenizer(en_sentence) + ['<eos>']\n",
    "        \n",
    "        temp_dict = {\"hi_tokens\": hi_tokens, \"en_tokens\": en_tokens, \"hi_len\":len(hi_tokens), \"en_len\":len(en_tokens)}\n",
    "        data_list.append(temp_dict)\n",
    "\n",
    "    #add extra sentences with one <unk> token just to make sure size of each batch equals batch_size.\n",
    "    #Even these sentences should include <sos> at the start and <eos> at the end \n",
    "    while len(data_list) % batch_size != 0:\n",
    "        temp_dict = {\"hi_tokens\": [\"<sos>\",\"<unk>\",\"<eos>\"], \"en_tokens\": [\"<sos>\",\"<unk>\",\"<eos>\"], \"hi_len\": 3, \"en_len\": 3}\n",
    "        data_list.insert(0,temp_dict)\n",
    "   \n",
    "    #sort according to length of hindi sentence\n",
    "    sorted_data = sorted(data_list, key= lambda x: x[\"hi_len\"])\n",
    "    return sorted_data\n",
    "\n",
    "sorted_data = sort_sentences(train_data,\"hi_len\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-RvGZvIU3wv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0liioK0HZ02K"
   },
   "outputs": [],
   "source": [
    "def create_batches(sorted_data,batch_size):     \n",
    "    batch_list = list()\n",
    "\n",
    "    #divide into batches\n",
    "    for i in range(0,len(sorted_data),batch_size):     \n",
    "        batch = sorted_data[i : i+batch_size]\n",
    "\n",
    "        #get the maximum length of sentences in the batch \n",
    "        hi_max_len = max(batch, key= lambda x: x[\"hi_len\"])[\"hi_len\"]\n",
    "        en_max_len = max(batch, key= lambda x: x[\"en_len\"])[\"en_len\"]\n",
    "        \n",
    "        temp_batch = list()\n",
    "        for item in batch:\n",
    "            hi_tokens, en_tokens, hi_len, en_len = item.values()\n",
    "            \n",
    "            #add <pad> tokens at the end of sentence (after <eos> token) to make sure \n",
    "            #each sentence in the batch has same length\n",
    "            if hi_len < hi_max_len:\n",
    "                padding = [\"<pad>\"]*(hi_max_len-len(hi_tokens))\n",
    "                item[\"hi_tokens\"] = hi_tokens + padding\n",
    "                \n",
    "            if en_len < en_max_len:\n",
    "                padding = [\"<pad>\"]*(en_max_len-len(en_tokens))\n",
    "                item[\"en_tokens\"] = en_tokens + padding\n",
    "\n",
    "            #convert token list into tensor    \n",
    "            hi_tensor = hindi.tokens2tensor(item[\"hi_tokens\"])\n",
    "            en_tensor = english.tokens2tensor(item[\"en_tokens\"])\n",
    "\n",
    "            #add the resultant tensors of both the languages into the batch\n",
    "            temp_item = [hi_tensor, en_tensor]\n",
    "            temp_batch.append(temp_item)\n",
    "        \n",
    "        #concat tensors such that the first row contains <sos> token of all sentences, \n",
    "        #2nd row contains the 1st word of all the sentences, and so on.\n",
    "        #In other words, ith column is for ith sentence of the batch\n",
    "        batch_hi = torch.cat([x[0] for x in temp_batch], dim=1)\n",
    "        batch_eng = torch.cat([x[1] for x in temp_batch], dim=1)\n",
    "        \n",
    "        #add the batch tensors into batch_list\n",
    "        batch_list.append([batch_hi, batch_eng])\n",
    "            \n",
    "    return batch_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7beauM0XlLBp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1Vr_dyHcM_m"
   },
   "outputs": [],
   "source": [
    "batch_list = create_batches(sorted_data, batch_size)\n",
    "\n",
    "#shuffle the batches\n",
    "random.shuffle(batch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zc6_DjBR2-Bi"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba_SRy1wE3Zc"
   },
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3J0LzVJ3CHB"
   },
   "outputs": [],
   "source": [
    "class EncoderGRU(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, n_layers, dropout):\n",
    "        super(EncoderGRU,self).__init__()\n",
    "        self.hidden_size = hidden_size          \n",
    "        self.input_size = input_size            #size of hindi vocab, i.e., unique tokens in hindi sentences \n",
    "        self.embedding_size = embedding_size    #embedding dimension \n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size,n_layers , dropout=dropout,bidirectional=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_sentence):\n",
    "        \n",
    "        embedding = self.embedding(input_sentence)   \n",
    "        embedding = self.dropout(embedding) \n",
    "        output, hidden= self.gru(embedding)          #gru returns 2 values: output and hidden state \n",
    "        '''\n",
    "         embedding.shape = [sentence_length, batch_size, embedding_size]\n",
    "         input_sentence.shape = [sentence_length,batch_size]\n",
    "         output.shape = [sentence_length,batch_size, n_directions * hidden_size)\n",
    "         hidden.shape = [n_directions*n_layers,batch_size,hidden_size]\n",
    "        '''\n",
    "        return output,hidden                        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMMZytUrFAHr"
   },
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVxMC_fU4BaB"
   },
   "outputs": [],
   "source": [
    "class DecoderGRU(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, n_layers,dropout):\n",
    "        super(DecoderGRU,self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size  \n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size,n_layers , dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #input.shape = [batch_size]           \n",
    "        input = input.unsqueeze(0)                      #added 1 dimension, input.shape = [1,batch_size]  \n",
    "        embedding = self.embedding(input)               #embedding.shape = [1, batch_size , embedding_size]\n",
    "        embedding = self.dropout(embedding)     \n",
    "        output, hidden = self.gru(embedding,hidden)  \n",
    "        '''\n",
    "         output.shape = [sentence_length,batch_size, n_directions * hidden_size)\n",
    "         hidden.shape = [n_directions*n_layers,batch_size,hidden_size]\n",
    "        '''   \n",
    "        predicted_output = self.linear(output)         \n",
    "        predicted_output = predicted_output.squeeze(0)   \n",
    "        return predicted_output,hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afvV1EVTlOHQ"
   },
   "source": [
    "#### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bID4KBeV4l2y"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,input_size,output_size,embedding_size,hidden_size,n_layers,dropout, device):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderGRU(input_size,embedding_size,hidden_size,n_layers,dropout).to(device)\n",
    "        self.decoder = DecoderGRU(output_size,embedding_size,hidden_size,output_size, n_layers,dropout).to(device)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self,input_sentence, target_output_sentence, teacher_forcing_ratio = 0.5):\n",
    "        input_sen_len = input_sentence.shape[0]\n",
    "        batch_size = input_sentence.shape[1]\n",
    "        output_sen_len = target_output_sentence.shape[0]\n",
    "        output_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        #tensor to store predictions by the decoder\n",
    "        predicted_word_indexes = torch.zeros(output_sen_len, batch_size, output_vocab_size).to(self.device)\n",
    "\n",
    "        #pass the input hindi sentence into the encoder \n",
    "        output, hidden = self.encoder(input_sentence)\n",
    "\n",
    "        decoder_input = target_output_sentence[0]  #first input to the decoder is always the init_token, i.e., <sos> token\n",
    "        \n",
    "        for i in range(1,output_sen_len):\n",
    "            #pass the previous word along with the hidden and cell states of encoder into the decoder\n",
    "            output,hidden = self.decoder(decoder_input, hidden)\n",
    "\n",
    "            #append the next prediction\n",
    "            predicted_word_indexes[i] = output\n",
    "\n",
    "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "            best_word = output.argmax(1)\n",
    "            if use_teacher_forcing:\n",
    "              decoder_input = target_output_sentence[i]\n",
    "            else:\n",
    "              decoder_input = best_word\n",
    "\n",
    "        return predicted_word_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDamH_ad4C8t"
   },
   "source": [
    "# Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nrSOYMfBqQ3"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH =400\n",
    "\n",
    "#hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "hidden_size = 512\n",
    "embedding_size = 256           #same for both grus (encoder and decoder)\n",
    "dropout = 0.5\n",
    "n_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-vG-Aomql7Sy"
   },
   "outputs": [],
   "source": [
    "#initialize the object of Seq2Seq class\n",
    "model = Seq2Seq(input_size,output_size,embedding_size,hidden_size,n_layers,dropout, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuX2k7zkV8jO"
   },
   "source": [
    "The parameter initialization has been done as per the paper :  https://arxiv.org/pdf/1409.3215.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "viR1g5G8GhjK",
    "outputId": "8a886257-eebf-4bcd-e8ca-1b8590518dba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): EncoderGRU(\n",
       "    (embedding): Embedding(41722, 256)\n",
       "    (gru): GRU(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): DecoderGRU(\n",
       "    (embedding): Embedding(32954, 256)\n",
       "    (gru): GRU(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (linear): Linear(in_features=512, out_features=32954, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize the parameters\n",
    "def init_weights(model):\n",
    "    for name, parameter in model.named_parameters():\n",
    "        nn.init.uniform_(parameter.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dA6WyL76kQrD"
   },
   "outputs": [],
   "source": [
    "def train_batch_list(model, batch_list, criterion,encoder_optimizer,decoder_optimizer):\n",
    "    for batch in batch_list:\n",
    "        #get hindi and their corresponding english sentences from the batch\n",
    "        input_sentence = batch[0]\n",
    "        target_sentence = batch[1]\n",
    "      \n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()    \n",
    "\n",
    "        #pass the hindi and their corresponding english sentences into the model to get the predicted sentence     \n",
    "        predicted_sentence = model(input_sentence, target_sentence) \n",
    "\n",
    "        #adjust the shapes\n",
    "        predicted_sentence = predicted_sentence[1:].view(-1, predicted_sentence.shape[2])\n",
    "        target_sentence = target_sentence[1:].view(-1)\n",
    "\n",
    "        #calculate loss    \n",
    "        loss = criterion(predicted_sentence,target_sentence)\n",
    "        \n",
    "        #backpropagate loss\n",
    "        loss.backward()\n",
    "\n",
    "        #clip the gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        #opitimize the parameters according to the propagated loss\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgH4P1jxGwO6"
   },
   "outputs": [],
   "source": [
    "def train(model, batch_list,num_epochs=num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    #initialize the optimizer and the criterion(Loss function) to be used\n",
    "    encoder_optimizer = optim.Adam(model.encoder.parameters(),lr=learning_rate)       #using Adam optimizer for encoder \n",
    "    decoder_optimizer = optim.Adam(model.decoder.parameters(),lr=learning_rate)       #using Adam optimizer for decoder\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = english.word2index[\"<pad>\"])       #using CrossEntropyLoss function\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        #calculate loss for epoch\n",
    "        loss = train_batch_list(model,batch_list,criterion,encoder_optimizer,decoder_optimizer)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        #save model\n",
    "        torch.save(model.state_dict(),\"gru_model_final.pt\")\n",
    "        \n",
    "        print(f'\\n\\nEpoch: {epoch+1}/{num_epochs}     Loss: {loss.item():.4f}')  \n",
    "\n",
    "    print(f\"\\n\\n Total loss ::: {total_loss/len(batch_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HpSQxuQmrIy5",
    "outputId": "93563af1-08fb-45b9-bb56-609ef0ede14d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 1/20     Loss: 4.7025\n",
      "\n",
      "\n",
      "Epoch: 2/20     Loss: 4.0282\n",
      "\n",
      "\n",
      "Epoch: 3/20     Loss: 3.9276\n",
      "\n",
      "\n",
      "Epoch: 4/20     Loss: 3.5618\n",
      "\n",
      "\n",
      "Epoch: 5/20     Loss: 2.9035\n",
      "\n",
      "\n",
      "Epoch: 6/20     Loss: 3.1232\n",
      "\n",
      "\n",
      "Epoch: 7/20     Loss: 3.0042\n",
      "\n",
      "\n",
      "Epoch: 8/20     Loss: 3.0170\n",
      "\n",
      "\n",
      "Epoch: 9/20     Loss: 2.8822\n",
      "\n",
      "\n",
      "Epoch: 10/20     Loss: 2.8076\n",
      "\n",
      "\n",
      "Epoch: 11/20     Loss: 2.5878\n",
      "\n",
      "\n",
      "Epoch: 12/20     Loss: 2.6990\n",
      "\n",
      "\n",
      "Epoch: 13/20     Loss: 2.5750\n",
      "\n",
      "\n",
      "Epoch: 14/20     Loss: 2.4953\n",
      "\n",
      "\n",
      "Epoch: 15/20     Loss: 2.3794\n",
      "\n",
      "\n",
      "Epoch: 16/20     Loss: 2.2575\n",
      "\n",
      "\n",
      "Epoch: 17/20     Loss: 2.4298\n",
      "\n",
      "\n",
      "Epoch: 18/20     Loss: 2.5494\n",
      "\n",
      "\n",
      "Epoch: 19/20     Loss: 2.2929\n",
      "\n",
      "\n",
      "Epoch: 20/20     Loss: 2.1526\n",
      "\n",
      "\n",
      " Total loss ::: 0.0184\n"
     ]
    }
   ],
   "source": [
    "train(model,batch_list, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QctI3W5rF1np"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwqawHHw4Mtx"
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "torch.save(model.state_dict(),\"gru_model_final.pt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdNoJ9AyG_Q8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBQI_ZrcAFal",
    "outputId": "3f0e66d7-7710-48f4-dd92-60964a98d5bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7iP8kUIJoqe"
   },
   "source": [
    "# Test on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9-9dWrO_Xtt",
    "outputId": "2425b0f2-4e36-4e18-8349-d78580c3e223"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): EncoderGRU(\n",
       "    (embedding): Embedding(41722, 256)\n",
       "    (gru): GRU(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): DecoderGRU(\n",
       "    (embedding): Embedding(32954, 256)\n",
       "    (gru): GRU(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (linear): Linear(in_features=512, out_features=32954, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the saved model\n",
    "# model = Seq2Seq(input_size,output_size,embedding_size,hidden_size,n_layers,dropout, device).to(device)\n",
    "# model.load_state_dict(torch.load(\"/content/drive/MyDrive/gru_model_final.pt\",map_location=torch.device(device)))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znS5L_X1ofSd"
   },
   "outputs": [],
   "source": [
    "def translate_sentence(model, hi_sentence, hindi, english, device, max_length=400):\n",
    "    #tokenize the hindi sentence\n",
    "    hi_tokens = hi_tokenizer(hi_sentence)\n",
    "\n",
    "    #convert it into tensor\n",
    "    sentence_tensor = hindi.tokens2tensor(hi_tokens)   \n",
    "\n",
    "    with torch.no_grad():\n",
    "        #pass the source sentence into the encoder to get the hidden and cell states\n",
    "        output,hidden = model.encoder(sentence_tensor)\n",
    "\n",
    "    predicted_word_indices = [english.word2index[\"<sos>\"]]              #index of <sos> in english vocab\n",
    "    predicted_sentence = \"\"\n",
    "\n",
    "    #repeat until the len of predicted sentence is less than max_length or the decoder predicts <eos>\n",
    "    while len(predicted_word_indices)<max_length and predicted_word_indices[-1]!= english.word2index[\"<eos>\"]:\n",
    "        prev_word = [predicted_word_indices[-1]]             \n",
    "        prev_word = torch.tensor(prev_word).to(device)                  #convert into tensor\n",
    "        best_word = \"\"\n",
    "        with torch.no_grad():\n",
    "            '''\n",
    "             pass the last predicted word along with the hidden and cell state of the encoder\n",
    "             into the decoder to get the next predicted word\n",
    "            '''\n",
    "            output,hidden = model.decoder(prev_word, hidden)\n",
    "            _ ,best_word = output.data.topk(1)                          #get the best predicted word index\n",
    "          \n",
    "        predicted_word_indices.append(best_word.item())                 #append it to the list of predicted word indices\n",
    "        predicted_sentence += english.index2word[best_word.item()]+\" \"  #append the word corresponding to the predicted index\n",
    "\n",
    "    translated_sentence = predicted_sentence.replace(\"<sos>\",\"\").replace(\"<eos>\",\"\")\n",
    "    return translated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "LwstSlILDe0v",
    "outputId": "a6f256f3-0dc9-45f8-885e-2e2540b72c73"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'hindi'"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = list()\n",
    "with open('testhindistatements.csv') as csvfile:\n",
    "     spamreader = csv.reader(csvfile, delimiter=\",\")\n",
    "     for row in spamreader:\n",
    "         test_data.append(row[2])\n",
    "test_data.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sLSGU3nKL7Hq",
    "outputId": "872b6142-6f1a-4685-c9a0-81c1261fbd9e"
   },
   "outputs": [],
   "source": [
    "# test_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feKABcS2dBVy"
   },
   "outputs": [],
   "source": [
    "predictions = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGjwO7GML8nT",
    "outputId": "7e8d53af-048e-498e-ee8c-3ff19a549932"
   },
   "outputs": [],
   "source": [
    "for i in range(len(test_data)):\n",
    "  #get the hindi sentence\n",
    "  sentence = test_data[i]\n",
    "\n",
    "  #get the predicted translated sentence\n",
    "  predicted_sentence = translate_sentence(model,sentence,hindi,english,device,max_length=400)\n",
    "  \n",
    "  #print(sentence,\"\\n\",predicted_sentence,\"\\n\\n\")\n",
    "\n",
    "  #append the results\n",
    "  predictions.append(predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6yuoVmKMTKS"
   },
   "outputs": [],
   "source": [
    "#write results into answer\n",
    "file = open(\"answer.txt\",\"w\")\n",
    "for x in predictions[:-1]:\n",
    "   file.write(x)\n",
    "   file.write(\"\\n\")\n",
    "file.write(predictions[-1])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nal4Z3B9Mqpr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxVN7_DU5D_V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1nnEWWA8SeH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UCu7Rl0DBgvu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DojduqllnEt1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdtHDYFAiHhe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgIRgEexIk9B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d42W9WUGRZzo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTKzMEkgRizW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niXRWDWaoQtH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vpPYpb7oRHa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "LevTeEwpDR93",
    "2Bw1m62tDhbj"
   ],
   "name": "Seq2seq with GRU (finalphase)(1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
